{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 07:18:34,541 - INFO - Device: cuda\n",
      "2025-01-21 07:18:34,541 - INFO - Model Path: checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "2025-01-21 07:18:34,542 - INFO - Output Directory: temp_test/\n",
      "2025-01-21 07:18:34,543 - INFO - Data Directory: /home/shashank/Documents/UniBonn/Sem5/aria-stereo-depth-completion/Priliminary tests/test_rectified_upright\n",
      "2025-01-21 07:18:34,544 - INFO - Filelist: ['/home/shashank/Documents/UniBonn/Sem5/aria-stereo-depth-completion/Priliminary tests/test_rectified_upright/aria1_r_rectified_upright.jpeg', '/home/shashank/Documents/UniBonn/Sem5/aria-stereo-depth-completion/Priliminary tests/test_rectified_upright/aria1_l_rectified_upright.jpeg']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading a list of 2 images\n",
      " - adding /home/shashank/Documents/UniBonn/Sem5/aria-stereo-depth-completion/Priliminary tests/test_rectified_upright/aria1_r_rectified_upright.jpeg with resolution 512x512 --> 512x384\n",
      " - adding /home/shashank/Documents/UniBonn/Sem5/aria-stereo-depth-completion/Priliminary tests/test_rectified_upright/aria1_l_rectified_upright.jpeg with resolution 512x512 --> 512x384\n",
      " (Found 2 images)\n",
      ">> Inference with model on 2 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode GlobalAlignerMode.PairViewer\n",
      "  - conf=7.46 for edge 0-1\n",
      "  - conf=10.8 for edge 1-0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PairViewer' object has no attribute 'preset_pose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilelist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilelist\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Get the reconstructed scene\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m scene, outfile, imgs \u001b[38;5;241m=\u001b[39m \u001b[43mget_reconstructed_scene_with_known_poses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSILENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCHEDULE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNITER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMIN_CONF_THR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAS_POINTCLOUD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMASK_SKY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLEAN_DEPTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRANSPARENT_CAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAM_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCENEGRAPH_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWINSIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKNOWN_POSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Output is .glb file\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UniBonn/Sem5/aria-stereo-depth-completion/other_projects/dust3r/dust3r/demo.py:242\u001b[0m, in \u001b[0;36mget_reconstructed_scene_with_known_poses\u001b[0;34m(outdir, model, device, silent, image_size, filelist, schedule, niter, min_conf_thr, as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size, scenegraph_type, winsize, refid, known_poses)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode)\n\u001b[1;32m    241\u001b[0m scene \u001b[38;5;241m=\u001b[39m global_aligner(output, device\u001b[38;5;241m=\u001b[39mdevice, mode\u001b[38;5;241m=\u001b[39mmode, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m silent)\n\u001b[0;32m--> 242\u001b[0m \u001b[43mscene\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreset_pose\u001b[49m(known_poses)\n\u001b[1;32m    243\u001b[0m scene\u001b[38;5;241m.\u001b[39mse\n\u001b[1;32m    244\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dust3r/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PairViewer' object has no attribute 'preset_pose'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import config as cfg\n",
    "from dust3r.model import AsymmetricCroCo3DStereo \n",
    "import logging \n",
    "from dust3r.demo import get_reconstructed_scene, get_reconstructed_scene_with_known_poses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level to INFO\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Define the output format\n",
    ")\n",
    "\n",
    "device = cfg.DEVICE \n",
    "weights_path = cfg.MODEL_PATH   \n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(weights_path).to(device)\n",
    "img_size = cfg.IMAGE_SIZE\n",
    "\n",
    "# Log the device, and model path\n",
    "logging.info(f\"Device: {device}\")\n",
    "logging.info(f\"Model Path: {weights_path}\")\n",
    "\n",
    "# Output directory \n",
    "output_dir = cfg.OUTPUT_DIR \n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "logging.info(f\"Output Directory: {output_dir}\")\n",
    "\n",
    "# Data directory\n",
    "data_dir = cfg.DATA_DIR\n",
    "logging.info(f\"Data Directory: {data_dir}\")\n",
    "filelist = os.listdir(data_dir) #return full path\n",
    "filelist = [os.path.join(data_dir, x) for x in filelist]\n",
    "logging.info(f\"Filelist: {filelist}\")\n",
    "\n",
    "\n",
    "# Get the reconstructed scene\n",
    "scene, outfile, imgs = get_reconstructed_scene_with_known_poses(\n",
    "                            output_dir, model, device, cfg.SILENT, img_size, filelist, cfg.SCHEDULE,\n",
    "                            cfg.NITER, cfg.MIN_CONF_THR, cfg.AS_POINTCLOUD, cfg.MASK_SKY,\n",
    "                            cfg.CLEAN_DEPTH, cfg.TRANSPARENT_CAMS, cfg.CAM_SIZE, cfg.SCENEGRAPH_TYPE,\n",
    "                            cfg.WINSIZE, cfg.REFID, cfg.KNOWN_POSES)\n",
    "\n",
    "# Output is .glb file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"images shape\", len(imgs))\n",
    "# print(\"First set of images shape\", imgs[0].shape, imgs[1].shape, imgs[2].shape, imgs[3].shape, imgs[4].shape, imgs[5].shape)\n",
    "# num_images = int(len(imgs)/3)\n",
    "# for i in range(num_images):\n",
    "#     ## Visualize the image, depth and confidence maps  at 3n-2, 3n-2, 3n indices\n",
    "#     img = imgs[3*i]\n",
    "#     depth = imgs[3*i+1]\n",
    "#     confidence = imgs[3*i+2]\n",
    "#     # Print max value in the depth map \n",
    "#     print(\"Max value in the depth map\", depth.max())\n",
    "#     print(\"Min value in the depth map\", depth.min())    \n",
    "#     print(f\"Image {i}\")\n",
    "#     print(f\"Image shape: {img.shape}\")\n",
    "#     print(f\"Depth shape: {depth.shape}\")\n",
    "#     print(f\"Confidence shape: {confidence.shape}\")\n",
    "#     print(\"\\n\")\n",
    "#     # Visualize the image, depth and confidence maps in a single plot \n",
    "#     fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "#     ax[0].imshow(img)\n",
    "#     ax[0].set_title(\"Image\")\n",
    "#     ax[1].imshow(depth)\n",
    "#     ax[1].set_title(\"Depth\")\n",
    "#     ax[2].imshow(confidence)\n",
    "#     ax[2].set_title(\"Confidence\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Visualize the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scene\", scene.get_im_poses())\n",
    "print(\"Scene\", scene.get_im_poses().shape)\n",
    "pose1 = scene.get_im_poses()[0] \n",
    "pose2 = scene.get_im_poses()[1]\n",
    "# Find relative pose between two cameras \n",
    "def relative_pose(pose1, pose2):\n",
    "    ''' \n",
    "    4*4 pose matrix of camera 1 and camera 2\n",
    "    return the relative pose between camera 1 and camera 2 \n",
    "    '''\n",
    "    poseR1 = pose1[:3,:3]\n",
    "    poseR2 = pose2[:3,:3]\n",
    "    poseT1 = pose1[:3,3]\n",
    "    poseT2 = pose2[:3,3]\n",
    "    relative_pose = pose2\n",
    "    relative_pose[:3,:3] = poseR1.T @ poseR2\n",
    "    relative_pose[:3,3] = poseT2 - poseT1\n",
    "    return relative_pose\n",
    "\n",
    "relative_pose = relative_pose(pose1, pose2)\n",
    "print(\"Relative Pose\", relative_pose)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_translation_rotation(matrix):\n",
    "\n",
    "    try:\n",
    "        matrix = matrix.detach().cpu().numpy()  # Move to CPU and convert to NumPy array\n",
    "    except:\n",
    "        # If the matrix is already a NumPy array\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Extract translation (last column)\n",
    "    translation = matrix[:3, 3]\n",
    "    translation = np.linalg.norm(translation)  # Compute the norm of the translation vector\n",
    "    \n",
    "    # Extract rotation (top-left 3x3 submatrix)\n",
    "    rotation_matrix = matrix[:3, :3]\n",
    "    \n",
    "    # Convert rotation matrix to Euler angles (yaw, pitch, roll)\n",
    "    yaw = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])  # Yaw (Z-axis)\n",
    "    pitch = np.arcsin(-rotation_matrix[2, 0])  # Pitch (Y-axis)\n",
    "    roll = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])  # Roll (X-axis)\n",
    "    # Convert angles to degrees\n",
    "    yaw = np.degrees(yaw)\n",
    "    pitch = np.degrees(pitch)\n",
    "    roll = np.degrees(roll)\n",
    "    \n",
    "    \n",
    "    return translation, yaw, pitch, roll\n",
    "\n",
    "# Extract translation and rotation from the relative pose\n",
    "relative_translation, relative_yaw, relative_pitch, relative_roll = extract_translation_rotation(relative_pose)\n",
    "print(\"Relative Translation\", relative_translation)\n",
    "print(\"Relative Yaw\", relative_yaw)\n",
    "print(\"Relative Pitch\", relative_pitch)\n",
    "print(\"Relative Roll\", relative_roll)\n",
    "\n",
    "\n",
    "# Test by transforming the pose1 to pose2\n",
    "pose1_transformed = relative_pose @ pose1\n",
    "print(\"Pose1 transformed\", pose1_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_relative_pose_rgb_l = [[ 0.99339134 -0.05150564  0.10257099 -0.00428072]\n",
    "#  [ 0.10364322  0.78649219 -0.60884162 -0.01184173]\n",
    "#  [-0.04931251  0.61544878  0.78663275 -0.00511398]\n",
    "#  [ 0.          0.          0.          1.        ]]\n",
    "\n",
    "# gt_relative_pose_r_l = [[ 0.99836471 -0.03532317  0.04494634  0.00567676]\n",
    "#  [ 0.05208901  0.23820291 -0.96981757 -0.11215309]\n",
    "#  [ 0.02355068  0.97057285  0.23965332 -0.08683892]\n",
    "#  [ 0.          0.          0.          1.        ]]\n",
    "import numpy as np\n",
    "gt_relative_pose_r_l = np.array([[ 0.99836471, -0.03532317,  0.04494634,  0.00567676],\n",
    "    [ 0.05208901,  0.23820291, -0.96981757, -0.11215309],\n",
    "    [ 0.02355068,  0.97057285,  0.23965332, -0.08683892],\n",
    "    [ 0.,          0.,          0.,          1.        ]])\n",
    "\n",
    "gt_relative_translation, gt_relative_yaw, gt_relative_pitch, gt_relative_roll = extract_translation_rotation(gt_relative_pose_r_l)\n",
    "print(\"GT Relative Translation\", gt_relative_translation)\n",
    "print(\"GT Relative Yaw\", gt_relative_yaw)\n",
    "print(\"GT Relative Pitch\", gt_relative_pitch)\n",
    "print(\"GT Relative Roll\", gt_relative_roll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_poses = [[[1.0, 0.0, 0.0, 0.0],\n",
    "               [0.0, 1.0, 0.0, 0.0],\n",
    "               [0.0, 0.0, 1.0, 0.0],\n",
    "               [0.0, 0.0, 0.0, 1.0]], \n",
    "                [[0.99339134, -0.05150564,  0.10257099, -0.00428072],\n",
    "                [ 0.10364322,  0.78649219, -0.60884162, -0.01184173],\n",
    "                [-0.04931251,  0.61544878,  0.78663275, -0.00511398],\n",
    "                [ 0.0, 0.0, 0.0, 1.0]]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Load the .glb file\n",
    "mesh = o3d.io.read_triangle_mesh(\"temp_test/scene.glb\")\n",
    "\n",
    "\n",
    "# Visualize the .glb file\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
